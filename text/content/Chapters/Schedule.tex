% Chapter 7

\chapter{Schedule} % Chapter title

\label{ch:schedule} % For referencing the chapter elsewhere, use \autoref{ch:examples} 

%----------------------------------------------------------------------------------------

As described in the preliminary results, a working prototype for
\ac{PASP} bottom-up compilation approach has been developed. 
Even though functional, the prototype has shown to be too slow 
for a realistic application, taking at least twice the time that
\textit{c2d} uses for its naive compilation. When considering
more structured approaches, with a better ordering for the 
Shannon decomposition in \textit{c2d}, the difference is even
larger, with the prototype taking up to hundreds or thousands
times more to compile; and obtaining circuits that are larger
than the ones produced by \textit{c2d}.

This bottleneck indicates two main problems with the current
implementation: the choice of the language (currently Julia,
utilizing the \textit{LogicCircuits.jl} package), and the lack
of better \textit{V-trees}, which can be improved by using
an static heuristic initialization and dynamic reordering
\citep{darwiche2011sdd, choi2013dynamic}.

Although the language of choice can be considered a mere 
difference of tooling, the usage of the \textit{SDD} package
(developed by Darwiche), written in $C$, enables the usage of
libraries such as \textit{pthreads} for parallelizing the 
disjunction of body rules, conjunction of the \acp{SDD} 
resulting from compiling each head, or even both.

As any problem characterized by parallelism, the naive
parallelization of all tasks (conjuntcions, disjunctions and
implications) can lead to an opposite effect, where context
switching and synchronization overheads can make the compilation
slower than the sequential version \citep{xu2007load}. With this
in mind, it is possible to take inspiration from simple
\textit{divide and conquer} algorithms, such as the 
\textit{merge sort} algorithm, which can be parallelized
easily by dividing the input in half, sorting each half. In the
case pf \ac{PASP}, the disjunction of body rules could be 
divided, with each thread compiling a subset of the body rules.
Another example would be to divide the conjunction of the 
resulting \ac{SDD} from compiling each head, with each thread.

This suggestion is still naive, since it does not describe any
type of control of the creation of the threads. One possible
solution would be the usage of a \textit{load balancer}
\citep{xu2007load, jeon2003parallel}, to avoid unnecessary
waiting, and to avoid the overhead of context switches due to
the system scheduler.

Another optimization would be to consider the \textit{AM-GM}
inequality. This claim is deeply connected to the complexity of
the \textit{apply} operation for \acp{SDD}, which belongs to the
complexity class $O(|\alpha| \cdot |\beta|)$, where $\alpha$ and
$\beta$ are the respective sizes of \acp{SDD} for which the
$apply$ operation is being applied. This indicates that, by
creating a thread for each head, and then using each thread to
compile the body rules sequentially, one could benefit from the
complexity of the $apply$ operation, since each body rule would
result in a small circuit, but it would be disjoined with a 
larger circuit representing the disjunction of all body rules.
Afterward, the conjunction of the resulting \acp{SDD} for each
thread could be done sequentially, to exploit the same 
complexity of the $apply$ operation.

\section{Schedule}

Overall, the main goal of the project is to (re)implement the
\acl{KC} algorithms, as described in \autoref{ch:contribution}
in a more efficient way, by using static and dynamic heuristics
for the \textit{V-trees}. Other important optimization to be
further investigated is the development an $X$-deterministic
algorithm, for building more efficient \acp{SDD} for the
\ac{2AMC} approach, since this relaxation would enable the usage
of more relaxed \textit{V-trees}.

If time allows, the project will also delve into a more 
fine-grained parallelization of the \ac{PASP} compilation.
Therefore, Table \ref{tab:schedule} describes, in a high-level,
the schedule for the project, containing each topic that will be
studied and implemented, and the respective months in which this
will be done.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \multirow{2}{*}{Task} & \multicolumn{7}{c|}{Month} \\ 
        \cline{2-8} & Jun & Jul & Aug & Sep & Oct & Nov & Dec  \\ \hline
        Static Heuristic for \textit{V-tree} Initialization & $\times$ & $\times$ & & & & & \\ \hline
        Dynamic Re-Ordenation of \textit{V-tree} & & $\times$ & $\times$ & & & & \\ \hline
        $X$-determinism & & & & $\times$ & $\times$ & & \\ \hline
        Code Parallelization & & & & & & $\times$ & \\ \hline
        Thesis Defense & & & & & & $\times$ & \\ \hline
    \end{tabular}
    \caption{Schedule for the project.}
    \label{tab:schedule}
\end{table}

For example, Static Heuristic for \textit{V-tree} Initialization
comprises the study and adaptation of the heuristics described
in \citep{darwiche2011sdd}. On the other hand, Dynamic 
Re-Ordenation of \textit{V-tree} and $X$-deteminism comprises 
the study and adaptation of the works \citep{choi2013dynamic} 
and \citep{choi2022solving}, respectively.

Finally, the Code Parallelization task is an optional task,
that will only be done if time allows and all previous tasks
were completed successfully. After all tasks are completed,
the defense of the thesis shall be scheduled for the month of
November.


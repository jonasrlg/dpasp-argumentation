% Chapter 5

\chapter{Literature Review} % Chapter title

\label{ch:literature_review} % For referencing the chapter elsewhere, use \autoref{ch:mathtest}

%----------------------------------------------------------------------------------------
The problem of efficiently computing
probabilistic inferences over a propositional ASP theory is of great
importance for Neuro-Symbolic systems. This is due to the inherent
complexity of computing Weighted Model Counting (WMC) queries and the necessity of
iterating over Answer Sets \citep{geh2023dpasp, yang2023neurasp,
EITER2024104109}. Furthermore, by representing the output of
Neural Networks as \textit{Annotated Disjunctions} \citep{geh2023dpasp,
yang2023neurasp}, one can integrate domain knowledge
through ASP rules with the representational capabilities of
Neural Networks for tasks such as \textit{Computer Vision} and
\textit{Natural Language Processing}.

This combination of logic and neural networks is not new,
as it has been explored in the literature for a variety of
applications, such as \textit{Inductive Logic Programming}
\citep{avila1999connectionist}. Consequently, many techniques have
been developed to perform inference over PLPs more
efficiently, such as \textit{lifted inference}
\citep{van2021introduction, totis2023lifted}, solution
enumeration by optimality \citep{pajunen2021solution,
manhaeve2021approximate}, and Knowledge Compilation.

With this in mind, we present in this chapter a review of the
current literature on PLP Knowledge Compilation. We first focus on the
ProbLog system and subsequently present a new approach for
PASP, which reduces the problem of computing WMC
queries to an extension of the Algebraic Model Counting (AMC) problem
\citep{kimmig2017algebraic}.

\section{Clark's Completion}

Before delving into the details of the Knowledge Compilation methods in the
literature, it is important to note that many top-down
compilation methods work by translating a PLP program into
a CNF formula, which is then compiled into a circuit. This
final step is usually performed by a knowledge compiler, such as
\textsc{c2d} or \textsc{d-Sharp}, both of which are based on the
sd-DNNF class of circuits \citep{darwiche2004new,
Muise2012}. For now, we will pause this discussion about knowledge
compilers and begin by describing the translation of a PLP
into a CNF.

To perform such a translation of a program into a CNF
formula, the most common approach is to use \textit{Clark's
Completion}, one of the main methods for attributing
meaning to negation in logic, similar to the Stable Models
semantics presented in Chapter \ref{ch:pasp}. This approach
differs from the Stable Models semantics in that the latter is
based on the idea of selecting some models of the
program to define its semantics, whereas the former focuses on
translating programs into a first-order theory, referred to as a
\textit{completion} of the program \citep{cozman2017semantics}.

\begin{definition}[Clark's Completion]
    Given a PLP program $P$, the Clark's completion of $P$
    is defined as the following propositional formula:
    \begin{equation}
        \textsc{Clark}(P) = \bigwedge_{a \in \mathcal{A}(P)}
        \left[a \iff \bigvee_{r \in \mathcal{R}(P,a)}
        \bigwedge_{b \in \textsc{body}(r)} b \right],
        \label{eq:clark_completion}
    \end{equation}
    where $\mathcal{A}(P)$ is the set of propositional atoms
    that appear in $P$, $\mathcal{R}(P,a)$ is the set of rules
    in $P$ that have $a$ as the head, and $\textsc{body}(r)$ is
    the set of literals in the body of rule $r$.
\end{definition}

This method can be transformed into a CNF formula, often with the addition of auxiliary
variables $aux_r$ for each rule $r$ in a program $P$. The resulting
CNF is the conjunction of the clauses described by Equations
\eqref{eq:clark_cnf_1} and \eqref{eq:clark_cnf_2}:

\begin{equation}
    \left[\bigwedge_{a \in \mathcal{A}(P)} \neg a \lor \left(
    \bigvee_{r \in \mathcal{R}(P,a)} aux_r \right)
    \right] \land \left[\bigwedge_{a \in \mathcal{A}(P)}
    \bigwedge_{r \in \mathcal{R}(P,a)} a \lor \neg aux_r\right]
    \label{eq:clark_cnf_1}
\end{equation}

\begin{equation}
    \bigwedge_{r \in P} aux_r \land \left[\bigwedge_{r \in P}
    \bigvee_{b \in \textsc{body}(r)} \neg b \right] \land \left[
    \bigwedge_{r \in P} \bigvee_{b \in \textsc{body}(r)} b \lor
    \neg aux_r \right]
    \label{eq:clark_cnf_2}
\end{equation}

It is important to note that, in order to control the number of
auxiliary variables created, a Treewidth-Aware method was
developed for Clark's completion \citep{fandinno2023treewidth}.
Together with another Treewidth-Aware algorithm for
(positive) cycle-breaking \citep{eiter2021treewidth},
these two methods are able to reduce the number of auxiliary
variables created when translating a program into a CNF
formula \citep{EITER2024104109, eiter2021treewidth}.

\section{ProbLog Compilation}

The Problog is one of the most well-known Probabilistic Logic Programming systems in the
literature and is based on the widely recognized Prolog
language. One of its main characteristics, and the most significant
difference in relation to PASP, is that it follows the
notion of \textit{stratified} programs, where the program is not
permitted to have \textit{negative} cycles in the dependency
graph.

With nearly two decades of research \citep{de2007problog}, this
PLP system has more than one version \citep{de2007problog,
fierens2015inference} and several extensions
\citep{totis2023smproblog, manhaeve2018deepproblog}. Thus, we
focus on describing the second version of Problog, two of its
compilation techniques, and one of its most recent extensions.

\subsection{ProbLog 2}

The main contribution of the second version of the ProbLog system was
the development of different KC algorithms for exact and
approximate inference. Since the approximate inference relies
on Monte Carlo Markov Chain methods, we focus on the
exact ones, which are based on solving the WMC using Knowledge Compilation.

Among the two algorithms proposed, there were:
\textit{top-down} and \textit{bottom-up} compilation of
CNFs, using d-DNNFs and BDDs, respectively. Both
were based on the following algorithm:

\begin{enumerate}
    \item First, the input program is parsed and grounded;
    \item Then, the grounded program is converted into a CNF;
    \item Finally, a \textit{knowledge compiler} is used to
    compile the CNF into a circuit.
\end{enumerate}

The main difference between the two algorithms was that the
former utilized the \textsc{c2d} compiler, which is based
on sd-DNNFs, while the latter relied on BDDs, hence
its capability of performing bottom-up compilation of a
CNF formula through compositions of the $apply$ operator.

\subsection{Bottom-Up Compilation}

To avoid the creation of auxiliary variables through
Clark's completion, the authors proposed a bottom-up
algorithm for compiling a PLP program \citep{vlasselaer2014compiling},
where one iteratively compiles the formula representing
\eqref{eq:clark_completion} by conjoining the circuits representing each
\textit{if and only if}.

As we will see in more detail in Chapter
\ref{ch:contribution}, by compiling all bodies of rules that
share the same head and \textit{disjoining} their respective
circuits, the authors are able to avoid the creation of the
$aux_i$ variables in Equations \eqref{eq:clark_cnf_1} and
\eqref{eq:clark_cnf_2}.

This is a similar approach to the work done in Problog's
second version, where bottom-up compilation performed
by BDDs is achieved by \textit{disjoining} and
\textit{conjoining} atoms. The main difference between these two
methods is that bottom-up compilation is able to skip
the translation to a CNF formula and is performed using SDDs,
a more succinct representation of circuits than BDDs (as
seen in \eqref{eq:succinctness_hierarchy}).

\subsection{$T_\mathcal{P}$ Compilation}

An advancement in the compilation of PLP for stratified programs is the
$T_\mathcal{P}$ compilation, proposed by \citep{vlasselaer2016tp}, where program compilation is performed
iteratively rather than in its entirety.

When compiling a program $P$ using the algorithm described in the previous
subsection, one would need to ground the program and apply a cycle-breaking
algorithm to compile the program in its entirety, rule by rule. The
$T_\mathcal{P}$ compilation, however, relaxes this constraint by allowing
partial compilation of a program or enabling exact compilation when a fixed
point of the iterative process is reached. This approximate compilation has a
wide range of applications in cases where an exact answer to the WMC task
is not necessary, but a quick computation is beneficial (within an acceptable
error margin, which the algorithm can control through lower and upper bounds).

The core idea of this algorithm lies in a generalization of the previously
described $T_\mathcal{P}$ operator in Chapter \ref{ch:pasp}, called the
$T_{C_\mathcal{P}}$ operator, which is defined as follows:

\begin{definition}[Parameterized Interpretation]
    Let $P$ be a \textit{ground} PLP with probabilistic facts $\mathcal{F}$
    and atoms $\mathcal{A}$. We say that $I$ is a parameterized interpretation
    of $P$ if $I = \set{(a, \lambda_a) | a \in \mathcal{A}}$, where $\lambda_a$
    is a propositional formula over $\mathcal{F}$ expressing in which
    interpretations $a$ is true.
\end{definition}

\begin{definition}[$T_{C_\mathcal{P}}$ Operator]
    Let $P$ be a \textit{ground} definite Probabilistic Logic Programming with probabilistic facts
    $F$, atoms $\mathcal{A}$, and a parameterized interpretation $I = \set{(a,
    \lambda_a)}$. Then, the $T_{C_\mathcal{P}}$ operator is of the form
    $T_{C_\mathcal{P}}(P) = \set{(a, \lambda'_a) | a \in \mathcal{A}}$, where
    \begin{displaymath}
       \lambda'_a =
       \begin{cases}
            a & \text{if $a$ is a probabilistic fact} \\
            \bigvee_{r \in \mathcal{R}_a} \bigwedge_{b \in \mathcal{B}_r}
            \lambda_b & \text{otherwise}
       \end{cases}
    \end{displaymath}
    $\mathcal{R}_a$ is the set of rules of $P$ where $a$ is the head, and
    $\mathcal{B}_r$ is the set of atoms in the body of a rule $r$.
\end{definition}

After defining the $T_{C_\mathcal{P}}$ operator above, the authors generalize it
for Normal Logic Programs and describe a simple algorithm based on the idea of
computing, for each iteration of the algorithm, the operator
$T_{C_\mathcal{P}}$ over one specific atom $a$, only considering the rules where
$a$ is the head. Hence, for each iteration, only the formula for the atom $a$ is
updated, and the algorithm stops when a fixed point is reached.

One way to represent these formulas in the interpretation $I$, which is
iteratively updated, is through the use of PSDDs, due to their ability to
perform Boolean operations efficiently \citep{darwiche2011sdd,
kisa2014probabilistic}. Therefore, one could replace the propositional formulas
$\lambda_a$ with PSDDs to perform a tractable computation of the
$T_\mathcal{P}$ compilation.

\subsection{smProblog Compilation}

Although ProbLog \citep{de2007problog, fierens2015inference} is a well-known
\textbf{stratified} Probabilistic Logic Programming system, it was recently extended to
\textbf{non-stratified} programs under the Stable Model semantics through the
development of \textsc{smProblog} \citep{totis2023smproblog}. With this novel
approach, the authors consider both the \textit{Stable Model} and \textsc{MaxEnt}
semantics (closely tied to the problem this work aims to address) and propose an
algorithm for performing both Knowledge Compilation and inference over these extended
Problog programs.

The need for a novel algorithm arises from the conceptual difference between the
evaluation of the $SUCC$ and $SUCC^{\mathrm{Max\text{-}Ent}}$ queries, which we
define as follows. The $SUCC$ problem for a query $q$ and a propositional theory
$T$ is defined as follows \citep{kiesel2022efficient}:

$$SUCC(q) = \sum_{I \in \mathcal{M}(T|q)} \mathbb{P}(I)
          = \sum_{I \in \mathcal{M}(T|q)} \prod_{i \in I}
          \alpha(i),$$

where $\mathcal{M}(T|q)$ is the set of models of $T$ that satisfy $q$, and
$P(I)$ is the probability of the models $I$ where $q$ \textit{succeeds}. Thus,
the $SUCC$ problem describes the sum of the probabilities of the models where
the query $q$ succeeds.

On the other hand, when considering probabilistic inference with \textit{Stable Models} and the
\textsc{MaxEnt} semantics, the $SUCC$ problem cannot be modeled only by considering
the models of a given theory, but rather the \textit{Stable Models} of the
program. Therefore, the $SUCC^{\mathrm{Max\text{-}Ent}}$ problem is defined as
follows \citep{kiesel2022efficient}:

$$SUCC^{\mathrm{Max\text{-}Ent}}(q) = \sum_{f \in \mathcal{A}(F)}
    \sum_{I \in \mathcal{M}(T|f)} \frac{|\mathcal{M}(T|f,q)|}
    {|M(T|f)|} \prod_{i \in I} \alpha(i),$$
where $F$ corresponds to the set of probabilistic facts of the program.

To address this normalization by the number of \textit{Stable Models},
characteristic of the \textsc{MaxEnt} semantics, the authors developed a
normalization step after performing the Knowledge Compilation process. In more detail, the
authors used a \textit{top-down} compilation approach, similar to the ProbLog 2
system, where the program is translated into a CNF and then compiled
through the use of a knowledge compiler (in this case, \textsc{d-Sharp}). They
then perform a normalization step by computing the number of \textit{Stable
Models} per total choice.

This normalization operation is particularly costly because it needs to replace
conjunctions and disjunctions of the circuit with Cartesian Products and Unions
to enumerate all the possible \textit{Stable Models} of the program and group
them by the total choices. Furthermore, both the Cartesian Product and the Union
operations are known to exhibit a higher computational cost than the former
(which can be computed in linear time), as they entail set manipulations, which
are more costly than manipulations of Boolean values. The Cartesian Product is
especially costly due to the need to compute the product of each element of the
sets involved in the operation.

\section{Second-Level Algebraic Model Counting}

To address this probabilistic inference problem via Knowledge Compilation in the context of PASP, the
current state-of-the-art approach consists of reducing a PASP inference
task to an instance of the Second Level Algebraic Model Counting (2AMC) problem
\citep{wang2025, kiesel2022efficient}.

Formally, the AMC problem is a generalization of different (counting) logic
problems, such as \textsc{SAT}, \textsc{MAX-SAT}, or WMC, where one describes a
\textit{semiring} over the set of models of a propositional theory, and the task
is to compute the value of a given formula under this semiring. For example, the
\textit{Tropical Semiring} is a well-known semiring that generalizes the
\textsc{MAX-SAT} as an AMC instance.

The 2AMC acts as an extension of the AMC, where a solution of an
\textit{inner} AMC is fed into an \textit{outer} AMC problem, which is
closely related to the notion of inference in PASP, as one needs to compute
an inference over \textit{Stable Models} of \textit{total choices}.

\subsection{Algebraic Answer Set Counting}

Under this general framework for Algebraic Answer Set Counting (AASC)
(instances of the 2AMC applied to PASP inference), inference can be tractably performed when imposing two
properties: $X$-\textit{firstness} and $X$-\textit{determinism} (which coincide
in SDDs, in a property called $X$-constraindness \citep{oztok2016solving})
\citep{wang2025}, defined as follows:

\begin{definition}[$X$-firstness]
    Let $\sigma$ be a NNF $n$ on variables $V$ partitioned into sets $X$
    and $Y$. Then, we say that an internal node $n_i$ of $\Sigma$ is
    \textit{pure} if its variables are present exclusively in $X$ or $Y$,
    $\mathrm{vars}(n_i) \subseteq X$ or $\mathrm{vars}(n_i) \subseteq Y$; and
    are called \textit{mixed} otherwise. Furthermore, we say that $n$ is
    $X$-\textit{first} if, for each of its \textit{and-nodes} $n_i$, all of its
    children are pure; or exactly one child is mixed and the remaining children
    $n_j$ are pure with $\mathrm{vars}(n_j) \subseteq X$.
\end{definition}

\begin{definition}[$X$-determinism]
    Let $\sigma$ be a NNF $n$ on variables $V$ partitioned into sets $X$
    and $Y$. Then, we say that an OR-node $n_i$ of $\Sigma$ is $X$-deterministic
    if, for any partial assignment $a$ of variables in $X$, we have that at most
    one of the children of $n_i$ has a nonzero output. If all of the OR-nodes in
    $\Sigma$ follow this property, then we say that $\Sigma$ is
    $X$-deterministic.
\end{definition}

With such properties, \citep{EITER2024104109} combine the 2AMC top-down
compilation via sd-DNNFs with treewidth-aware methods, resulting in the
following algorithm:

\begin{enumerate}
    \item Given a propositional program, a (positive) cycle-breaking algorithm
    is applied, which has an upper bound based on the treewidth
    \citep{eiter2021treewidth}.
    \item Next, the cycle-free program is converted into a CNF formula via
    another treewidth-based algorithm, this time based on Clark's completion
    \citep{fandinno2023treewidth}.
    \item Finally, the CNF formula is compiled through the use of a
    knowledge compiler (\textsc{c2d}), and inference can be performed directly
    via model counting, which involves an inner model counting over answer sets
    and an outer weighted model counting over total choices.
\end{enumerate}

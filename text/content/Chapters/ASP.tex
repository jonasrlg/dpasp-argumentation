% Appendix Answer Set Programming

\chapter{Answer Set Programming}
\label{ch:asp}

Knowledge Representation (KR) research is driven by a familiar tension:
expressive formalisms capture rich domains but typically incur intractable
inference, while restricted languages admit efficient reasoning at the cost of
what can be expressed. Full first-order logic, for example, allows us to encode
complex knowledge bases but entailment in this setting is undecidable in
general and, even for decidable fragments, often demands exponential resources
\citep{levesque1986knowledge}. Despite numerous refinements, classical
resolution-based procedures inherit this worst-case behaviour.

One way to regain tractability is to move to less expressive yet carefully
designed fragments. Propositional logic exemplifies this strategy: by grounding
away quantifiers we can apply SAT technology, as discussed in
Chapter~\ref{ch:sat}. Answer Set Programming (ASP) follows the same philosophy.
It restricts syntax in a way that keeps many modelling conveniences—default
negation, recursion, and non-monotonic reasoning—while providing a semantics
that supports efficient solver implementations. ASP has become a workhorse for
applications ranging from planning to computational argumentation
\citep{toni2011argumentation}.

This chapter reviews the logical foundations required for the probabilistic ASP
encodings that we adopt later. We begin with definite programs and the
associated semantic machinery, introduce stable-model semantics through loop
formulas and related constructs, and then extend the language with features
such as disjunction and cardinality constraints. These ingredients will be
essential when we encode bipolar argumentation in Chapter~\ref{ch:contribution}.
%----------------------------------------------------------------------------------------
\section{Foundations of Logic Programming} \label{sec:lp}

\subsection{Definite Logic Programs}

Before diving on the semantics of Probabilistic Answer Set Programming (PASP), it is important to
understand less expressive formalisms, such as \textit{definite}
and \textit{propositional} programs. These formalisms are not only
the basis where Answer Set Programming (ASP) is built upon, but also hint at the
underlying complexity of the problems that Answer Set Programming (ASP) can solve
and the efficiency of the algorithms that can be used in this
context. Thus, we start by defining the simplest form of
Logic Programming (LP), called \textit{definite} (or \textit{positive})
programs:

\begin{definition}[Definite Logic Programs]
    A definite logic program $P$ is a finite set of clauses
    (rules) in the form
    \begin{minted}{prolog}
        a :- b1, ..., bM.
    \end{minted}
    where $a, b_1, \ldots, b_M$ are atoms of a function-free
    FOL $L$; and this rule can be seen as material
    implication restricted to Horn clauses, where
    \mintinline{prolog}{a :- b1, ..., bM} is read as $B \supset
    A$ or $B \rightarrow A$ \citep{eiter2009answer}. The atom
    $a$ is called the \textit{head} of the rule, while $b_1,
    \ldots , b_m$ are called the rule’s \textit{body}. When a
    rule has an empty body, it is called a fact and can be
    shortened as $a$.

    The following program is an example of a definite program:
    \begin{minted}{prolog}
        happy(turing) :- friends(turing, vonNeumann).
    \end{minted}

    Programs without variables, like the one above, are called
    \textit{propositional} programs.
\end{definition}

We could skip the definition of propositional programs, if we
were not interested in the complexity of this problem, that is
not only $P-complete$, but can be solved in linear time
(testing satisfiability of propositional (Horn) formulas)
\citep{dowling1984linear}.

\subsection{Herbrand Universe, Base and Interpretation}

A natural question that arises after the definition of this new
class of program is about its complexity. This answer largely
depends on the type of normal program that we are dealing with.
In search of a more formal explanation, we have to introduce
more concepts to capture a class of programs that we are
comfortable between the trade-off of expressiveness and
efficiency. We do this by firstly defining the concept of
\textit{Herbrand Universe} and \textit{Base}:

\begin{definition}[Herbrand Universe]
    The \textit{Herbrand Universe} $HU(P)$ of a logic program $P$
    is the set of all terms that can be formed from constants
    and functions in $P$ (w.r.t. a predefined vocabulary $L$).
    Moreover, the \textit{Herbrand Base} $HB(P)$ of $P$ is the set
    of all ground atoms that can be formed from terms and
    predicates occurring $HU(P)$. Finally, the \textit{Herbrand
    Interpretation} is a subset of $HB(P)$, an interpretation
    $I$ (a set denoting ground \textit{truths}) over $HU(P)$.
\end{definition}

\begin{example}
    Consider the following logic program $P$
    \citep{eiter2009answer}:
    \begin{minted}{prolog}
        h(0, 0).
        t(a, b, r).
        p(0, 0, b).
        p(f(X), Y, Z) :- p(X, Y, Z'), h(X, Y), t(Z, Z', r).
        h(f(X), f(Y)) :- p(X, Y, Z'), h(X, Y), t(Z, Z', r).
    \end{minted}
    Then, the Herbrand Universe $HU(P)$ is the union of the set
    containing all constants of $P$, $\{0, a, b, r\}$, and the
    set of terms that can be formed from these constants
    $\{f(0), f(a), f(b), f(r), f(f(0)), f(f(a)), f(f(b)),
    f(f(r)), \ldots\}$. Whereas, the Herbrand Base, $HB(P)$ is
    given by the set of all ground atoms assertions, $\{p(0, 0,
    0), p(a, a, a), \ldots, h(0, 0), \ldots, t(0, 0, 0), t(a, a,
    a), \ldots\}$.

    Finally, we list a few Herbrand Interpretations over
    $HU(P)$:

    \begin{itemize}
        \item $I_1 = \emptyset$;
        \item $I_2 = HB(P)$;
        \item $I_3 = \{h(0, 0), t(a, b, r), p(0, 0, b)\}$.
    \end{itemize}
    Note that not all interpretations are consistent with the
    program $P$. For instance, $I_1$ is contradictory because it
    does not include any of the facts of $P$.
\end{example}

\subsection{Grounding of a Logic Program}

With the notion of Herbrand Universe and Base, we can now have
a moral formal definition of \textit{grounding}
\citep{eiter2009answer}:

\begin{definition}[Grounding]
    We define a ground instance of a clause $C$, of a logic
    program  $P$, as any clause $C'$ obtained from $C$ by
    applying a substitution
    \begin{displaymath}
        \theta: Var(C) \rightarrow HU(P),
    \end{displaymath}
    where $Var(C) \in \mathcal{V}(P)$ is the set of variables
    in $C$. Moreover, the grounding of a program $P$ is the set
    of all possible ground instances of the clauses in $P$ and
    is denoted by $ground(P) = \bigcup_{C \in P} ground(C)$ (the
    union of all ground instances for all the clauses in $P$).
\end{definition}

\subsection{Interpretation of a Logic Program}

As we are already formalizing the concepts like grounding, the
Interpretation of a logic program is also a concept that needs
more attention. Thus, follows its definition:

\begin{definition}[Interpretation]
    The interpretation $I$ of a logic program $P$ is a model of
    $P$ that is compatible with the assertions in $P$. That is,
    $I$ is a model of

   \begin{itemize}
        \item A ground clause $C =$ \mintinline{prolog}{a :- b1, ..., bM, not c1, ..., cN},
        denoted $I \models C$, if either $\Set{a, c_1, \ldots,
        c_N} \cap I \ne \emptyset$ or $\Set{b_1, \ldots, b_M}
        \not \subseteq I$;
        \item A clause $C$, denoted $I \models C$, if $I \models
        C'$ for every $C' \in ground(C)$;
        \item A program $P$, denoted $I \models P$, if
        $I \models C$ for every clause $C \in P$.
   \end{itemize}
   That is: an interpretation $I$ is a model of a program $P$ if
   it is compatible with all the ground instances of the clauses
   of $P$.

   We call a model $I$ of a program $P$ a \textit{minimal model},
   if there is no other model $J$ of $P$ such that $J \subset
   I$. Although, Normal logic programs can have multiple minimal models
   for a program, it is true that Definite Logic Programs only
   have one minimal model \citep{eiter2009answer}.
\end{definition}

\subsection{Loop Formulas}

\begin{definition}[Loops]
    Let $P$ be a normal logic program. Any nonempty subset $L$
    of the atoms of $P$ is called a \textit{loop} if for any two
    atoms $p,q$ in $L$, there is a path in the dependency graph
    of $P$ from $p$ to $q$ of length greater than zero.

    We also associate two sets of rules with a loop $L$:
    \begin{itemize}
        \item The set of rules $R^+(L,P)$ contains rules of $P$
        whose heads and bodies are in $L$; and
        \item The set of rules $R_L^-(L,P)$ contains rules about
        atoms in $L$ that are out of the loop $L$.
    \end{itemize}
\end{definition}

\begin{definition}[Loop Formulas]
    Let $P$ be a normal logic program and $L$ be a loop of $P$.
    Then, the \textit{loop formula} $LF(L,P)$ is the following
    implication:
    \begin{displaymath}
        \bigvee_{p \in L} \neg p \;\rightarrow\; \neg \bigwedge_{r
        \in R_L^{-}(L,P)} \mathrm{body}(r),
    \end{displaymath}
    where $\mathrm{body}(r)$ is the body of the rule $r$.
\end{definition}

The main idea behind loop formulas is that they can be used to
transform logic programs under stable model semantics to
propositional theories. This is particularly useful when these
propositional theories are fed into SAT solvers to compute
stable models. The entire process of translating a logic program
by this approach includes: using Clark's completion to create a
propositional theory and augment it by using additional loop
formulas, that guarantees that this theory admits only stable
models \citep{lin2004assat}. One problem with this approach is
that the number of loop formulas must be controlled in some
way, because there can be an exponential number of loop formulas
for a given program \citep{eiter2009answer}.

\section{Answer Set Programming} \label{sec:asp}

Finally, after defining the main semantics for logic programs
that include negation, we introduce the concept of ASP,
an extension of normal logic programming that allows for the use
of: integrity constraints, strong negation, disjunctive rules,
and choice rules \citep{eiter2009answer, maua2020complexity}.

\subsection{Extended Logic Programs}

We start this ASP definition by first introducing the
concept of Extended Logic Programs (ELP) and Extended Disjunctive
Logic Programs (EDLP), programs that use the three
extensions defined above.

\subsubsection{Integrity Constraints}

Integrity constraints rule out interpretations that violate hard requirements.
They are typically written as headless rules of the form

\begin{minted}{prolog}
    :- b1, ..., bM, not c1, ..., not cN.
\end{minted}

The same requirement can be expressed with a head by introducing an auxiliary
predicate:

\begin{minted}{prolog}
    falsity :- b1, ..., bM, not falsity, not c1, ..., not cN,
\end{minted}

where the auxiliary $falsity$ is a fresh propositional atom
\citep{eiter2009answer}.

\subsubsection{Strong Negation}

Strong negation is not strictly required—default negation combined with
integrity constraints can simulate it—but having a dedicated connective makes
programs clearer. While classical logic writes the fact that $a$ is known to be
false as $\neg a$, ASP customarily denotes it by $-a$.

\subsubsection{Extended Logic Programs}

By combining normal logic programs with integrity constraints
and strong negation, we are capable of defining ELP:

\begin{definition}[Extended Logic Programs]
    An extended logic program $P$ is a finite set of rules of
    the form:
    \begin{minted}{prolog}
        a :- b1, ..., bM, not c1, ..., not cN,
    \end{minted}
    where $M, N \geq 0$; $a, b_i, c_i$ are atoms or
    \textit{strongly negated} atoms of a FOL.
\end{definition}

Because integrity constraints and strong negation can be reduced to the basic
semantics, we do not need a new notion of model: stable models of extended
programs remain \emph{answer sets}.

\subsubsection{Disjunctive Logic Programs}

The last extension that we define for normal logic programs is:
the addition of disjunctions on the head of rules:
\begin{definition}[Extended Disjunctive Logic Programs]
    An extended disjunctive logic program $P$ is a finite set
    of rules of the form:
    \begin{minted}{prolog}
        a1 ; ... ; aK :- b1, ..., bM, not c1, ..., not cN,
    \end{minted}
    where $K, M, N \geq 0$; $a_i, b_i, c_i$ are atoms or
    \textit{strongly negated} atoms of a FOL; and the
    symbol $;$ denotes disjunction (similar to how commas
    denote conjunctions).
\end{definition}

The semantics of EDLPs mirror those of extended programs with two additions:
answer sets are now minimal models of the reduct $P^M$ (which may have multiple
minimal models), and Clark's completion must be generalised to cope with
disjunctive heads \citep{alviano2016completion}.

Finally, we recall the notion of interpretation for EDLPs, unifying the various
extensions that lead from normal logic programs to full ASP.

\begin{definition}[Interpretation (of an EDLP)]
    Let $P$ be an EDLP. An interpretation $I$ is a model
    of:
    \begin{enumerate}
        \item A ground clause $C =$ \mintinline{prolog}{a1; ...; aK :- b1, ..., bM, not c1, ..., cN},
        denoted $I \models C$, if either $\Set{a_1, \ldots, a_K,
        c_1, \ldots, c_N} \cap I \ne \emptyset$ or $\Set{b_1,
        \ldots b_M} \not \subseteq I$;
        \item A clause $C$, denoted $I \models C$, if $I \models
        C$ for every $C' \in ground(C)$; and
        \item A program $P$, denoted $I \models P$, if $I
        \models C$ for every clause $C \in P$.
    \end{enumerate}
\end{definition}

\subsection{Cardinality Constraints}

The last extension that characterises Answer Set Programming is the notion of
\textit{cardinality constraints}, constructs of the
form: \mintinline{prolog}{L \{l1, ..., ln \} U}, that are
satisfied whenever the number of satisfied literals $l_i$ is
between the integral bounds $L$ and $U$, inclusive
\citep{syrjanen2001smodels}. As Syrjänen and Niemelä described,
a cardinality constraint in a rule head imposes a
non-deterministic choice over the literals in it when the rule
body is satisfied.

\subsubsection{Choice Rules}

A special case of cardinality constraints are \textit{choice
rules}: rules where the head is enclosed in brackets and
represent the idea that the head can be included in a stable
model only if the body holds; but it can be left out, too
\citep{syrjanen2001smodels}. This type of rule can be expressed
through normal rules by introducing a new atom. For example,

\begin{example}
    The following program $P$
    \begin{minted}{prolog}
        {a} :- b, not c.
    \end{minted}
    is equivalent to the Normal logic program $P'$:
    \begin{minted}{prolog}
        a :- not aa, b, not c.
        aa :- not a.
    \end{minted}
\end{example}

\section{Probabilistic Logic Programming}

In order to define Probabilistic Answer Set Programming, we
delve into how logical programs are capable of augmenting
their capabilities to express probability distributions as
generative models. Sato's distribution semantics \citep{sato1995statistical}
is probably one of the most seminal works in this area, where
the idea behind it is to define a distribution semantics that associates
probability to a set of \textit{independent} events  in such a way that
a unique probability measure is induced over all interpretations of
ground atoms \citep{cozman2020joy}.

\begin{definition}[Probabilistic Fact]
    A probabilistic fact is a pair consisting of an atom $A$ and
    a probability value $\alpha$, which we denote by $\alpha ::
    A$ \citep{cozman2017semantics}.

    Additionally, atoms of probabilistic facts may contain logical
    variables ($\alpha :: r(X_1, \ldots, X_n)$), and are therefore
    unground. When considering this case, we interpret such a
    probabilistic fact as the set of all grounded probabilistic
    facts, obtained after grounding the variables in the atom.

    Finally, we say that probabilistic facts do not unify with the
    head of any rule. In other word, there is no substitution $\theta$
    that makes a probabilistic fact $f$ and a rule head $h$
    syntactically identical, we do not have $f\theta = h\theta$ for
    any value of $f$ and $h$.
\end{definition}

Note that this definition of PLPs only associates probabilities through the use
of probabilistic facts. Thus, properties from logic programs are also present in
PLPs, such as \textit{stratification}, \textit{acyclicity}, and
\textit{definiteness}.

In order to further extend this probabilistic semantics, one may define
multi-valued scenarios, where probabilities are not of the type
\mintinline{prolog}{p::a}, where $a$ has probability $p$ of being \emph{true}
and $(1-p)$ of being \emph{false}; but instead we have a distribution over many
scenarios $a(1), \ldots, a(k)$, each associated with a probability $p(i)$. This
notion, called \textit{annotated disjunctions}, extends the previous definition
of PLPs to Extended Disjunctive Logic Programming (EDLP), by adding
probabilistic facts to head of rules.

\begin{definition}[Annotated Disjunctive Rules]
    A probabilistic extension of an EDLP is called
    Annotated Disjunctive Rules (ADR), a set of rules of the form:
    \begin{minted}{prolog}
        p1::a(1); ...; pK::a(K) :- b1, ..., bM, not c1, ..., not cN,
    \end{minted}
    where $p_1, \ldots, p_k$ are nonnegative real values
    whose sum is smaller than or equal to 1. In this sense, a
    probabilistic fact of the form
    \begin{minted}{prolog}
        p::a
    \end{minted}
    is a special case (and a syntactic sugar) of an ADR, where
    \mintinline{prolog}{p::a} is equivalent to
    \mintinline{prolog}{p::a (1-p):-f}, where $f$ is an atom
    that is never true \citep{geh2023dpasp}.
\end{definition}

\subsection{Probabilistic Answer Set Programming}

Although we have briefly defined Sato's distribution semantics,
there is a need for a more formal description. Furthermore,
this distribution semantics that Sato proposed is not well
adapted to consider ASP programs.

It is important to note that we are defining a probability
distribution over PLPs, and not logical ones. This makes
probabilistic semantics over PLPs agnostic w.r.t. the
logical semantics of the program, which is particularly useful
when considering non-stratified programs that may use a
\textit{stable model} or \textit{well-founded} semantics
\citep{eiter2009answer, maua2020complexity}.

\begin{definition}[Probabilistic Choice]
    A PLP $(P, PF)$ with $n$ probabilistic facts can
    generate a total of $2^n$ different logic programs: one for
    each possible assignment of the probabilistic facts (when
    considering binary probabilistic facts). The assignment of
    a probabilistic fact $\alpha :: A$ can be seen as choosing
    to keep or erase the fact $A$ from the program
    \citep{cozman2017semantics}.

    As commented before, these \textit{probabilistic choices} are
    assumed to be independent.
\end{definition}

\begin{definition}[Total Choice]
    A \textit{total choice} $\theta$ for a PLP $(P, PF)$ is
    a subset of the set of grounded probabilistic facts. Thus,
    $\theta$ can be interpreted as a set of ground facts that
    are probabilistically selected to be included in $P$, while
    all other ground facts obtained from probabilistic facts are
    discarded \citep{cozman2017semantics}.

    Because the probabilistic choices are assumed to be
    independent, the probability of a total choice can be easily
    computed by the product over the grounded probabilistic
    facts, where a probabilistic fact $\alpha :: A$ that is
    chosen to be included contributes with a factor of $\alpha$,
    and, if it is discarded, it contributes with a factor of
    $(1-\alpha)$ \citep{cozman2017semantics}.

    An important property of total choices is that, for a total
    choice $\theta$, the induced logic program (w.r.t. $P$),
    denoted by $P \cap PF^{\downarrow \theta}$, is a normal
    logic program \citep{cozman2017semantics, geh2023dpasp}.
    Therefore, for an ADR, a total choice $\theta$ induces
    a normal logic program, where each rule $r$ from the
    ADR is transformed into a rule of the form
    \begin{minted}{prolog}
        a_i :- b1, ..., bM, not c1, ..., not cN,
    \end{minted}
    where $i$ is the corresponding index of the probabilistic
    choice in $\theta$ \citep{geh2023dpasp}.
\end{definition}

Whenever we have a total choice $\theta$ for a PLP such
that $P \cup PF^{\downarrow \theta}$ is stratified, the
resulting induced program has only one \textit{stable model}, as
we can conclude by earlier Subsections \ref{sub:stratification} and
\ref{sub:stable_model}. Hence, Sato's distribution semantics can be naturally
extended to this class of programs, and does not create any controversial
interpretation of the probabilistic semantics.

A more interesting case is when we have a total choice $\theta$
for a PLP that induces a (non-stratified) logical program
with multiple \textit{stable models}. In this case, we have the
need to define semantics capable of handling this situation:
the Credal and \textsc{MaxEnt} semantics, which we define next.

First, we define the notion of \textit{consistency} for
PLPs:

\begin{definition}[Consistency of Probabilistic Logic Programs]
    A PLP $(P, PF)$ is \textit{consistent} if, for each
    total choice $\theta$, the induced logic program $P \cup
    PF^{\downarrow \theta}$ has at least one stable model
    \citep{cozman2017semantics}.
\end{definition}

Given that we have a definition to express PLPs that are
able to have stable models, independent of the total choice, we
are capable of formalizing the notion of a \textit{probability
model} for PLPs:

\begin{definition}[Probability Model for Probabilistic Logic
    Programs]
    A \textit{probability model} for a consistent PLP $(P,
    PF)$ is a probability measure $\mathbb{P}$ over
    interpretations of $P$, such that
    \citep{cozman2017semantics}:
    \begin{enumerate}
        \item Every interpretation $I$ with $\mathbb{P}(I) > 0$
        is a stable model of (the normal logic program) $P \cup
        PF^{\downarrow \theta}$ w.r.t. the total choice $\theta$
        that agrees with $I$ on the probabilistic facts; and
        \item The probability of each total choice $\theta$ is
        the product of the probabilities for all individual
        choices in $\theta$:
        $$\mathbb{P}(\Set{I \mid I \cap C = C}) = \prod_{\alpha
        :: A | A \in C} \alpha \prod_{\alpha :: A | A \notin C}
        (1 - \alpha).$$
    \end{enumerate}
\end{definition}

\subsubsection{Credal Semantics}

As the first PASP semantics to be described, we introduce
the \textit{credal semantics}, which is based on the idea of
using intervals (containing \textit{upper} and \textit{lower})
to represent the uncertainty of the probability
\citep{lukasiewicz2005probabilistic,
lukasiewicz2007probabilistic}.

\begin{definition}[Credal Semantics]
    The \textit{credal semantics} for a consistent PLP $(P,
    PF)$ is the set of all probability models for $(P, PF)$
    \citep{cozman2017semantics}.

    Moreover, the credal semantics is a \textit{closed} and
    \textit{convex} set of probability measures, and corresponds
    to the set of all probability measures that dominate an
    infinitely monotone Choquet capacity
    \citep{cozman2017semantics} (more about Choquet capacities
    can be found in \cite{cozman2020joy}). As such, the credal
    semantics of any set of models $\mathcal{M}$ is
    characterized by an interval $[\underline{\mathbb{P}}(\mathcal{M}),
    \overline{\mathbb{P}}(\mathcal{M})]$:

    \begin{equation}
        \underline{\mathbb{P}}(\mathcal{M}) = \sum_{\theta \in
        \Theta : \Gamma(\theta) \subseteq \mathcal{M}} \mathbb{P}
        (\theta), \qquad
        \overline{\mathbb{P}}(\mathcal{M}) = \sum_{\theta \in \Theta
        : \Gamma(\theta) \cap \mathcal{M} \ne \emptyset} \mathbb{P}
        (\theta),
        \label{eq:credal_semantics}
    \end{equation}

    \noindent where $\Theta$ is the set of all total choices, and $\Gamma
    (\theta)$ is the set of stable models associated with the
    total choice \citep{cozman2017semantics, cozman2020joy}.

    Given a PLP, this interval can be computed for a set
    of assignments $\mathbf{Q}$ for ground atoms by the
    following algorithm \citep{cozman2017semantics}:

    \begin{enumerate}
        \item Given a PLP $(P, PF)$ and $\mathbf{Q}$,
        initialize variables $a$ and $b$ with 0;
        \item For each total choice $\theta$, compute the set $S$
        of all stable models of $P \cap PF^{\downarrow \theta}$,
        and:
            \begin{enumerate}
                \item If $\mathbf{Q}$ is \textit{true} in every
                stable model in $S$, then $a \leftarrow a + P(
                \theta)$; and
                \item If $\mathbf{Q}$ is \textit{true} in some
                stable model of $S$, then $b \leftarrow b + P(
                \theta)$.
            \end{enumerate}
        \item Return $[a, b]$ as the interval $[\underline{\mathbb{P}}(
        \mathbf{Q}),\overline{\mathbb{P}}(\mathbf{Q})]$.
    \end{enumerate}

    An interesting relation between the credal semantics and
    reasoning over Answer Sets is that the computation of the
    upper probability, $\overline{\mathbb{P}}(\mathbf{Q})$,
    involves \textit{brave reasoning}, and the computation of
    the lower probability, $\underline{\mathbb{P}}(\mathbf{Q})$,
    involves \textit{cautious reasoning}
    \citep{cozman2017semantics}.
\end{definition}

\subsubsection{Maximum-Entropy Semantics}

To conclude this chapter, we define one of the most famous
semantics for PASP: the \textit{Maximum
Entropy} semantics, also known as \textsc{MaxEnt} semantics:

\begin{definition}[Maximum Entropy (MaxEnt) Semantics]
    This semantics is based on the principle of maximum entropy:
    evenly dividing the probability mass among all stable
    models of a program (induced by a total choice). Formally,
    for a stable model $\mathcal{M}$, the probability
    $\mathbb{P}(\mathcal{M})$ is given by:

    $$\mathbb{P}(\mathcal{M}) = \sum_{\theta : \mathcal{M} \in
    \Gamma(\theta)} \frac{\mathbb{P}(\theta)}{n},
    $$

    where $n$ is the number of stable models associated with
    the total choice $\theta$ \citep{cozman2017semantics}.
\end{definition}

\subsection{PASP Inference as 2AMC}

By analyzing the definitions of PASP semantics, an immediate observation is that
they are closely related to the \textit{two-level weighted model counting}
(2AMC) problem defined in Chapter~\ref{ch:amc}. In fact, both the credal
semantics and the \textsc{MaxEnt} semantics can be reduced to instances of
2AMC \citep{kiesel2023knowledge, azzolini2023inference}. In more detail,
when defining the 2AMC framework, \citep{kiesel2023knowledge} showed that
the \textsc{MaxEnt} semantics can be directly mapped to a 2AMC instance by
having an inner model counting problem that counts the number of stable models
for each total choice, and an outer problem that sums the probabilities of
these total choices. Similarly, \citep{azzolini2023inference} demonstrated that
the credal semantics can also be expressed as a 2AMC problem, where the inner
problem applies \textit{brave}/\textit{cautious} reasoning to verify whether
the query holds in some/all stable models for each total choice, and the outer
problem sums the probabilities of these total choices, as a ``standard''
weighted model counting problem.

%---------------------------------------------------------------

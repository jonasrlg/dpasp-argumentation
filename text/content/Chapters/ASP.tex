% Chapter 4

\chapter{Answer Set Programming}
\label{ch:asp}

Knowledge Representation (KR) research is driven by a familiar tension:
expressive formalisms capture rich domains but typically incur intractable
inference, while restricted languages admit efficient reasoning at the cost of
what can be expressed. Full first-order logic, for example, allows us to encode
complex knowledge bases but entailment in this setting is undecidable in
general and, even for decidable fragments, often demands exponential resources
\citep{levesque1986knowledge}. Despite numerous refinements, classical
resolution-based procedures inherit this worst-case behaviour.

One way to regain tractability is to move to less expressive yet carefully
designed fragments. Propositional logic exemplifies this strategy: by grounding
away quantifiers we can apply SAT technology, as discussed in
Chapter~\ref{ch:sat}. Answer Set Programming (ASP) follows the same philosophy.
It restricts syntax in a way that keeps many modelling conveniences—default
negation, recursion, and non-monotonic reasoning—while providing a semantics
that supports efficient solver implementations. ASP has become a workhorse for
applications ranging from planning to computational argumentation
\citep{toni2011argumentation}.

This chapter reviews the logical foundations required for the probabilistic ASP
encodings that we adopt later. We begin with definite programs and the
associated semantic machinery, introduce stable-model semantics through loop
formulas and related constructs, and then extend the language with features
such as disjunction and cardinality constraints. These ingredients will be
essential when we encode bipolar argumentation in Chapter~\ref{ch:contribution}.
%----------------------------------------------------------------------------------------
\section{Foundations of Logic Programming} \label{sec:lp}

\subsection{Definite Logic Programs}

Before diving into the semantics of Probabilistic Answer Set Programming (PASP),
it is important to understand less expressive formalisms, such as \textit{definite}
and \textit{propositional} programs. These formalisms underpin Answer Set
Programming (ASP): they reveal the inherent complexity of the problems ASP can
solve and guide the design of efficient algorithms. We therefore begin by
defining the simplest form of logic programming, namely \textit{definite} (or
\textit{positive}) programs:

\begin{definition}[Definite Logic Programs]
    A definite logic program $P$ is a finite set of clauses
    (rules) in the form
    \begin{minted}{prolog}
        a :- b1, ..., bM.
    \end{minted}
    where $a, b_1, \ldots, b_M$ are atoms of a function-free
    FOL $L$; and this rule can be seen as material
    implication restricted to Horn clauses, where
    \mintinline{prolog}{a :- b1, ..., bM} is read as $B \supset
    A$ or $B \rightarrow A$ \citep{eiter2009answer}. The atom
    $a$ is called the \textit{head} of the rule, while $b_1,
    \ldots , b_m$ are called the rule’s \textit{body}. When a
    rule has an empty body, it is called a fact and can be
    shortened as $a$.

    The following program is an example of a definite program:
    \begin{minted}{prolog}
        happy(turing) :- friends(turing, vonNeumann).
    \end{minted}

    Programs without variables, like the one above, are called
    \textit{propositional} programs.
\end{definition}

Although we could omit propositional programs, they are instructive from a
complexity perspective: deciding satisfiability for propositional (Horn) programs
is $P$-complete, yet it admits linear-time algorithms
\citep{dowling1984linear}.

\subsection{Normal Logic Programs}

To reason about non-monotonic phenomena we allow default negation in rule
bodies, obtaining \textit{normal} programs:

\begin{definition}[Normal Logic Program]
    A normal logic program is a finite collection of rules written as
    \begin{minted}{prolog}
        a :- b1, ..., bM, not c1, ..., not cN.
    \end{minted}
    Here $M, N \ge 0$, each $a$, $b_i$, and $c_i$ is an atom over a function-free
    first-order language, and the keyword \texttt{not} denotes negation as failure
    ($\neg c$). We refer to the $b_i$ as positive subgoals and to the literals
    \texttt{not}~$c_i$ as negative subgoals. Empty bodies are omitted, yielding
    facts, and empty heads correspond to integrity constraints discussed later.
\end{definition}

\subsection{Herbrand Universe, Base and Interpretation}

The symbols occurring in a program determine a canonical universe over which we
evaluate rules. The two standard constructions are the \textit{Herbrand
universe} and the \textit{Herbrand base}.

\begin{definition}[Herbrand Universe]
    For a logic program $P$, the Herbrand universe $HU(P)$ is the set of all
    ground terms that can be assembled from the constants and function symbols
    appearing in $P$ (with respect to the background language $L$). The
    \textit{Herbrand base} $HB(P)$ consists of every predicate symbol of $P$
    applied to tuples of terms from $HU(P)$. Any subset $I \subseteq HB(P)$ is a
    Herbrand interpretation; its members are precisely the ground atoms that $I$
    designates as true.
\end{definition}

\begin{example}
    Consider the program $P$ from \cite{eiter2009answer}:
    \begin{minted}{prolog}
        h(0, 0).
        t(a, b, r).
        p(0, 0, b).
        p(f(X), Y, Z) :- p(X, Y, Z'), h(X, Y), t(Z, Z', r).
        h(f(X), f(Y)) :- p(X, Y, Z'), h(X, Y), t(Z, Z', r).
    \end{minted}
    The constants $\{0, a, b, r\}$ generate infinitely many ground terms through
    the unary symbol $f$, yielding a universe that contains $\{0, a, b, r, f(0),
    f(a), f(b), f(r), f(f(0)), \ldots\}$. Instantiating the predicates $h$, $t$,
    and $p$ with tuples drawn from $HU(P)$ forms $HB(P)$; for example,
    \texttt{p(0,0,0)}, \texttt{h(f(0),f(0))}, and \texttt{t(a,a,a)} are all
    included.

    Typical Herbrand interpretations include $\emptyset$, the full base $HB(P)$,
    or intermediate sets such as $\{h(0,0), t(a,b,r), p(0,0,b)\}$. The empty set
    is inconsistent with $P$ because it disregards the program’s facts.
\end{example}

\subsection{Grounding of a Logic Program}

Grounding replaces variables with ground terms taken from $HU(P)$.
\cite{eiter2009answer} formalise the construction as follows.

\begin{definition}[Grounding]
    Let $C$ be a clause in a program $P$. A ground instance of $C$ is obtained by
    applying a substitution
    \[
        \theta : \mathrm{Var}(C) \rightarrow HU(P).
    \]
    The set of all such instances is denoted $\mathrm{ground}(C)$. The grounding
    of $P$ is the union $\mathrm{ground}(P) = \bigcup_{C \in P} \mathrm{ground}(C)$,
    comprising every clause produced in this way.
\end{definition}

\subsection{Interpretation of a Logic Program}

Ground programs are evaluated with respect to Herbrand interpretations.

\begin{definition}[Interpretation]
    Let $P$ be a logic program. An interpretation $I \subseteq HB(P)$ is said to
    satisfy:
    \begin{itemize}
        \item A ground rule $C =$ \mintinline{prolog}{a :- b1, ..., bM, not c1, ..., not cN}
        when either one of $a, c_1, \ldots, c_N$ belongs to $I$ or not all of the
        atoms $b_1, \ldots, b_M$ are contained in $I$;
        \item A (possibly non-ground) clause $C$ if every ground instance $C' \in
        \mathrm{ground}(C)$ is satisfied by $I$; and
        \item The whole program $P$ when each clause in $P$ is satisfied.
    \end{itemize}

    Interpreting $I$ as a model of $P$ therefore amounts to checking compatibility
    with all ground rules. A model $I$ is \textit{minimal} if no proper subset of
    $I$ is also a model. Definite programs have a unique minimal model, while
    normal programs may have several \citep{eiter2009answer}.
\end{definition}

\subsection{Loop Formulas}

\begin{definition}[Loops]
    Given a normal logic program $P$, a non-empty set $L$ of atoms is a
    \textit{loop} when, for every pair $p, q \in L$, there exists a directed path
    in the dependency graph of $P$ from $p$ to $q$ of positive length.
    Associated with $L$ we consider two collections of rules:
    \begin{itemize}
        \item $R^+(L,P)$, comprising the rules in $P$ whose heads and bodies
        mention only atoms from $L$; and
        \item $R_L^-(L,P)$, covering rules whose heads belong to $L$ but whose
        bodies introduce atoms outside $L$.
    \end{itemize}
\end{definition}

\begin{definition}[Loop Formulas]
    For a loop $L$ in $P$, the corresponding loop formula is
    \[
        \bigvee_{p \in L} \neg p \;\rightarrow\; \neg \bigwedge_{r \in R_L^{-}(L,P)}
        \mathrm{body}(r),
    \]
    where $\mathrm{body}(r)$ denotes the conjunction of body literals in rule
    $r$.
\end{definition}

Loop formulas provide additional constraints that eliminate unfounded sets when
encoding stable models as SAT problems. In the ASSAT approach
\citep{lin2004assat}, Clark’s completion yields a propositional theory whose
models are filtered by iteratively adding loop formulas until only stable
models remain. Since the number of loops can be exponential, practical systems
must generate these formulas carefully \citep{eiter2009answer}.

\subsection{Immediate Consequence Operator}

Fixing notation for semantics, we employ the immediate consequence operator
$T_P$.

\begin{definition}[$T_P$ Operator]
    For a normal program $P$, the operator $T_P : 2^{HB(P)} \to 2^{HB(P)}$ maps a
    set of ground atoms $I$ to
    \[
        T_P(I) = \Bigl\{a \;\Bigm|\;
            \mintinline{prolog}{a :- b1, ..., bM, not c1, ..., not cN} \in
            \mathrm{ground}(P),
            \, \{b_1, \ldots, b_M\} \subseteq I,
            \, \{c_1, \ldots, c_N\} \cap I = \emptyset
        \Bigr\}.
    \]
\end{definition}

The operator is monotone \citep{bogaerts2015knowledge}. Hence, by the
Knaster–Tarski theorem it possesses a least fixed point $lfp(T_P)$, which
coincides with the least model of $P$ \citep{eiter2009answer}. Iterating $T_P$
from the empty interpretation converges to this fixed point.

\subsection{Negation and Stratification}
\label{sub:stratification}

Different families of logic programs balance expressiveness and computational
cost, but structural aspects also matter—especially in the presence of negation.
Negated literals introduce the possibility of multiple minimal models, so we
require semantics that clarify how to interpret such programs. Two broad
strategies are common:

\begin{itemize}
    \item Look for a single canonical model. This succeeds for \textit{stratified}
    programs and is exemplified by the well-founded semantics
    \citep{van1991well}, which we do not cover in detail.
    \item Accept the existence of several models and reason with all of them,
    which leads to stable-model-based approaches such as ASP.
\end{itemize}

One can detect stratification by constructing a total order on rule evaluation
so that negative literals only depend on strictly earlier strata
\citep{eiter2009answer}. The notion can be expressed using dependency graphs.

\begin{definition}[Dependency Graph]
    For a ground program $P$, the dependency graph $dep(P) = (V, E)$ contains a
    vertex for every ground atom in $P$. There is a directed edge $(v, w)$ when
    $v$ occurs in the head of a rule whose body mentions $w$. If $w$ appears
    negated, we annotate the edge with a mark $*(v, w)$ to indicate a negative
    dependency. A program is acyclic when this graph has no directed cycles.
\end{definition}

Using the graph we formalise stratification.

\begin{definition}[Stratification]
    Let $G = (V, E)$ be the dependency graph of $P$. A stratification is a
    partition $\Sigma = \{S_1, \ldots, S_n\}$ of the predicates of $P$ into
    non-empty, pairwise disjoint sets satisfying:
    \begin{enumerate}
        \item If $(v, w) \in E$ is a positive edge with $v \in S_i$ and $w \in
        S_j$, then $i \le j$; and
        \item If $*(v, w) \in E$ is a negative edge, $v \in S_i$, and $w \in
        S_j$, then $i > j$.
    \end{enumerate}
    Whenever such a partition exists we call $P$ \textit{stratified}, and the
    sets $S_i$ are the program’s strata.
\end{definition}

\subsection{Stable Model Semantics}
\label{sub:stable_model}

As noted earlier, normal programs need not possess a unique minimal model once
negation is allowed. The following example illustrates this behaviour.

\begin{example}
    Consider the program
    \begin{minted}{prolog}
        researcher(computability).
        machine(X) :- researcher(X), not lambda(X).
        lambda(X) :- researcher(X), not machine(X).
    \end{minted}
    The dependency graph contains a negative cycle; hence the program is not
    stratified. Two minimal models exist: $M_1 =
    \{\texttt{researcher(computability)}, \texttt{machine(computability)}\}$ and
    $M_2 = \{\texttt{researcher(computability)}, \texttt{lambda(computability)}\}$.
\end{example}

To attach a semantics to such programs we employ the Gelfond–Lifschitz reduct.

\begin{definition}[Reduct (GL-reduct)]
    Let $P$ be a normal program and $I$ an interpretation. The \textit{reduct} of
    $P$ with respect to $I$, written $P^I$, is constructed by
    \begin{enumerate}
        \item grounding $P$;
        \item discarding every rule whose body contains $\texttt{not } c$ with $c
        \in I$; and
        \item removing all remaining negative literals.
    \end{enumerate}
\end{definition}

Intuitively, the reduct treats atoms in $I$ as true. Any rule whose body assumes
their negation becomes unsatisfied and is removed. Rules that rely on negated
atoms outside $I$ retain their head but lose those negated literals. The result
$P^I$ is a positive program and therefore has a unique least model $LM(P^I)$. If
this least model coincides with $I$, the interpretation is stable.

\begin{definition}[Stable Model]
    An interpretation $I$ of a ground normal program $P$ is a stable model when
    $I$ is the minimal model of the reduct $P^I$.
\end{definition}

Stable models are fixed points of $T_P$: whenever $I$ is stable, $T_P(I) = I$,
though the converse does not hold in general \citep{eiter2009answer}. In this
chapter we follow ASP terminology and refer to stable models as \textit{answer
sets}.

\subsection{Reasoning}

Reasoning tasks for stable models can be organised by increasing difficulty:

\begin{enumerate}
    \item \textbf{Consistency}: determine whether the program admits at least
    one answer set.
    \item \textbf{Brave reasoning}: given a ground literal $Q$, decide if there
    exists an answer set in which $Q$ holds. Such literals are \textit{brave
    consequences}.
    \item \textbf{Cautious reasoning}: given $Q$, test whether every answer set
    entails $Q$. When this happens we say that $Q$ is a \textit{cautious
    consequence}.
\end{enumerate}

\section{Answer Set Programming} \label{sec:asp}

We now assemble the ingredients required for full Answer Set Programming. ASP
extends normal logic programs with integrity constraints, strong negation,
disjunctive heads, and cardinality constructs, while retaining stable-model
semantics \citep{eiter2009answer, maua2020complexity}.

\subsection{Extended Logic Programs}

We begin with \textit{extended logic programs} (ELPs) and \textit{extended
disjunctive logic programs} (EDLPs), which enrich normal programs with the new
constructs.

\subsubsection{Integrity Constraints}

Integrity constraints forbid answer sets in which certain combinations of body
literals hold. They are written as headless rules:

\begin{minted}{prolog}
    :- b1, ..., bM, not c1, ..., not cN.
\end{minted}

When convenient we introduce an auxiliary atom to encode the same restriction:

\begin{minted}{prolog}
    falsity :- b1, ..., bM, not falsity, not c1, ..., not cN.
\end{minted}

Here \texttt{falsity} is a fresh propositional symbol that never appears
elsewhere \citep{eiter2009answer}.

\subsubsection{Strong Negation}

Although strong negation can be simulated using default negation plus integrity
constraints, it is convenient to treat explicit falsity as a primitive. In ASP
syntax, the assertion that $a$ is provably false is written $-a$, mirroring the
classical connective $\neg a$.

\subsubsection{Extended Logic Programs}

By combining normal logic programs with integrity constraints
and strong negation, we are capable of defining ELP:

\begin{definition}[Extended Logic Programs]
    An extended logic program $P$ is a finite collection of rules of the form
    \begin{minted}{prolog}
        a :- b1, ..., bM, not c1, ..., not cN.
    \end{minted}
    Here $M, N \ge 0$ and every literal $a$, $b_i$, or $c_i$ may be either an
    atom or its strong negation within the underlying first-order language.
\end{definition}

Because integrity constraints and strong negation can be reduced to the basic
semantics, we do not need a new notion of model: stable models of extended
programs remain \emph{answer sets}.

\subsubsection{Disjunctive Logic Programs}

We further enhance expressiveness by allowing disjunctions in rule heads.

\begin{definition}[Extended Disjunctive Logic Programs]
    An extended disjunctive logic program $P$ comprises rules
    \begin{minted}{prolog}
        a1 ; ... ; aK :- b1, ..., bM, not c1, ..., not cN.
    \end{minted}
    with $K, M, N \ge 0$, where every literal $a_i$, $b_i$, and $c_i$ may be a
    strongly negated atom. The semicolon denotes disjunction, contrasting with
    the comma used for conjunction in rule bodies.
\end{definition}

EDLPs inherit stable-model semantics from extended programs. The reduct $P^M$
remains central, though it may now have several minimal models. Moreover,
Clark’s completion needs a disjunctive generalisation to capture supported
models \citep{alviano2016completion}.

To conclude, we recall the notion of interpretation for EDLPs, bridging the
language extensions and the semantics used by ASP solvers.

\begin{definition}[Interpretation (of an EDLP)]
    Let $P$ be an EDLP. An interpretation $I$ satisfies:
    \begin{enumerate}
        \item A ground rule $C =$ \mintinline{prolog}{a1; ...; aK :- b1, ..., bM, not c1, ..., not cN}
        whenever either one of the head literals $a_1, \ldots, a_K$ (or any
        negated literal $c_j$) belongs to $I$, or not all positive body atoms
        $b_1, \ldots, b_M$ are in $I$;
        \item A possibly non-ground clause $C$ if $I$ satisfies every ground
        instance $C' \in \mathrm{ground}(C)$; and
        \item The whole program $P$ if each rule of $P$ is satisfied.
    \end{enumerate}
\end{definition}

\subsection{Cardinality Constraints}

The final construct we consider is the \textit{cardinality constraint}
\citep{syrjanen2001smodels}. Expressions of the form
\mintinline{prolog}{L {l1, ..., ln} U} are true whenever exactly between $L$ and
$U$ of the literals $l_i$ hold. Placing such a constraint in a rule head
enforces a non-deterministic selection of literals once the body is satisfied.

\subsubsection{Choice Rules}

A prominent instance is the \textit{choice rule}, whose head is written in
braces. Intuitively, whenever the body succeeds we may choose to include the
head literal in the answer set, but omission remains possible
\citep{syrjanen2001smodels}. Choice rules can be compiled away by adding
auxiliary atoms:

\begin{example}
    The program
    \begin{minted}{prolog}
        {a} :- b, not c.
    \end{minted}
    behaves like the normal program
    \begin{minted}{prolog}
        a :- not aa, b, not c.
        aa :- not a.
    \end{minted}
\end{example}

\section{Probabilistic Logic Programming}

To define Probabilistic Answer Set Programming, we first review how logic
programs can be enriched to express probability distributions as generative
models. Sato's distribution semantics \citep{sato1995statistical} is one of the
seminal works in this area: it associates probability to a set of
\textit{independent} events in such a way that a unique probability measure is
induced over all interpretations of ground atoms \citep{cozman2020joy}.

\begin{definition}[Probabilistic Fact]
    A probabilistic fact pairs an atom $A$ with a probability value $\alpha$, a
    notation we abbreviate as $\alpha :: A$ \citep{cozman2017semantics}.

    The atom may contain variables—for instance $\alpha :: r(X_1, \ldots, X_n)$.
    In that case we interpret the declaration as shorthand for all of its ground
    instantiations. Moreover, probabilistic facts are required to be syntactically
    distinct from the heads of program rules: no substitution can turn a fact and
    a rule head into the same atom.
\end{definition}

Under this scheme probabilities enter exclusively through probabilistic facts.
Consequently, structural properties of the underlying logic program—such as
stratification, acyclicity, or definiteness—carry over to the probabilistic
setting.

To describe multi-valued random choices we use \textit{annotated disjunctions}.
Rather than assigning probability $p$ to a single fact $a$ (with $1-p$ on its
negation), we distribute mass across several mutually exclusive outcomes
$a(1), \ldots, a(k)$ with probabilities $p(1), \ldots, p(k)$. Annotated
disjunctions extend PLPs to the EDLP setting by placing probabilistic weights on
rule heads.

\begin{definition}[Annotated Disjunctive Rules]
    In an annotated disjunctive program each rule has the form
    \begin{minted}{prolog}
        p1::a(1); ...; pK::a(K) :- b1, ..., bM, not c1, ..., not cN.
    \end{minted}
    The non-negative weights $p_1, \ldots, p_K$ sum to at most one. The special
    case of a single annotated atom
    \begin{minted}{prolog}
        p::a.
    \end{minted}
    can be simulated by
    \mintinline{prolog}{p::a; (1-p)::dummy :- .}, where
    \texttt{dummy} is a fresh atom that never appears elsewhere
    \citep{geh2023dpasp}.
\end{definition}

\subsection{Probabilistic Answer Set Programming}

We now revisit Sato’s distribution semantics with the adaptations necessary for
stable-model reasoning. Probabilities are attached to the entire PLP, rather
than to individual derivations, which keeps the approach compatible with both
stable and well-founded semantics for non-stratified programs
\citep{eiter2009answer, maua2020complexity, geh2024dpasp}.

\begin{definition}[Probabilistic Choice]
    Suppose a PLP $(P, PF)$ contains $n$ probabilistic facts of the binary form
    $\alpha :: A$. Each combination of keeping or deleting these facts yields a
    different deterministic logic program, so there are $2^n$ possibilities in
    total \citep{cozman2017semantics}. We refer to the decision taken for each
    fact as a \textit{probabilistic choice}, and assume that all such choices are
    probabilistically independent.
\end{definition}

\begin{definition}[Total Choice]
    A \textit{total choice} $\theta$ for $(P, PF)$ selects a subset of the
    grounded probabilistic facts to be kept; the remaining ground instances are
    dropped from the program \citep{cozman2017semantics}. Independence implies
    that the probability of $\theta$ is simply the product of the individual
    contributions: each retained fact $\alpha :: A$ contributes $\alpha$, while
    each omitted fact contributes $(1-\alpha)$.

    The program determined by $\theta$, written $P \cap PF^{\downarrow \theta}$,
    is a normal logic program \citep{cozman2017semantics, geh2023dpasp}. In an
    annotated disjunctive rule, the choice of a head atom effectively selects
    the corresponding deterministic rule
    \begin{minted}{prolog}
        ai :- b1, ..., bM, not c1, ..., not cN.
    \end{minted}
\end{definition}

If a total choice $\theta$ yields a stratified program $P \cup PF^{\downarrow
\theta}$, the induced program has a unique stable model by
Section~\ref{sub:stable_model}. Sato’s semantics therefore extends smoothly to
this setting.

More subtle behaviour arises when some total choice leads to a non-stratified
program with several stable models. We handle this situation through the credal
and \textsc{MaxEnt} semantics introduced below.

First, we define the notion of \textit{consistency} for
PLPs:

\begin{definition}[Consistency of Probabilistic Logic Programs]
    A PLP $(P, PF)$ is \textit{consistent} when every total choice $\theta$
    induces at least one stable model for $P \cup PF^{\downarrow \theta}$
    \citep{cozman2017semantics}.
\end{definition}

Given that we have a definition to express PLPs that are
able to have stable models, independent of the total choice, we
are capable of formalizing the notion of a \textit{probability
model} for PLPs:

\begin{definition}[Probability Model for Probabilistic Logic Programs]
    A probability model for a consistent PLP $(P, PF)$ is a probability measure
    $\mathbb{P}$ over interpretations of $P$ satisfying
    \citep{cozman2017semantics}:
    \begin{enumerate}
        \item $\mathbb{P}(I) > 0$ only when $I$ is a stable model of the induced
        program $P \cup PF^{\downarrow \theta}$ for some total choice $\theta$
        that agrees with $I$ on the probabilistic facts; and
        \item The mass assigned to each total choice $\theta$ factorises as the
        product of the probabilities of its selections:
        \[
            \mathbb{P}(\{I \mid I \cap C = C\}) =
            \prod_{\alpha :: A,\, A \in C} \alpha
            \prod_{\alpha :: A,\, A \notin C} (1 - \alpha).
        \]
    \end{enumerate}
\end{definition}

\subsubsection{Credal Semantics}

Our first probabilistic semantics is the \textit{credal} approach, which
represents uncertainty through probability intervals
\citep{lukasiewicz2005probabilistic, lukasiewicz2007probabilistic}.

\begin{definition}[Credal Semantics]
    The credal semantics of a consistent PLP $(P, PF)$ is the set of all its
    probability models \citep{cozman2017semantics}. This set is closed and
    convex, and it coincides with the probability measures that dominate an
    infinitely monotone Choquet capacity \citep{cozman2017semantics,
    cozman2020joy}.

    For any collection of interpretations $\mathcal{M}$ we obtain an interval
    $[\underline{\mathbb{P}}(\mathcal{M}), \overline{\mathbb{P}}(\mathcal{M})]$,
    where
    \begin{equation}
        \underline{\mathbb{P}}(\mathcal{M}) =
        \sum_{\theta \in \Theta \,:\, \Gamma(\theta) \subseteq \mathcal{M}}
        \mathbb{P}(\theta), \qquad
        \overline{\mathbb{P}}(\mathcal{M}) =
        \sum_{\theta \in \Theta \,:\, \Gamma(\theta) \cap \mathcal{M} \ne \emptyset}
        \mathbb{P}(\theta),
        \label{eq:credal_semantics}
    \end{equation}
    $\Theta$ is the set of all total choices and $\Gamma(\theta)$ contains the
    stable models associated with $\theta$.

    An algorithmic view \citep{cozman2017semantics} for computing the interval of
    a ground query $\mathbf{Q}$ proceeds as follows:
    \begin{enumerate}
        \item Initialise $a = 0$ and $b = 0$.
        \item For every total choice $\theta$, enumerate all stable models $S$ of
        $P \cap PF^{\downarrow \theta}$ and update
            \begin{enumerate}
                \item $a \gets a + \mathbb{P}(\theta)$ if $\mathbf{Q}$ holds in
                every model of $S$; and
                \item $b \gets b + \mathbb{P}(\theta)$ if $\mathbf{Q}$ holds in
                some model of $S$.
            \end{enumerate}
        \item Return $[a, b]$ as
        $[\underline{\mathbb{P}}(\mathbf{Q}), \overline{\mathbb{P}}(\mathbf{Q})]$.
    \end{enumerate}

    Observe that $\overline{\mathbb{P}}(\mathbf{Q})$ corresponds to brave
    reasoning, whereas $\underline{\mathbb{P}}(\mathbf{Q})$ relies on cautious
    reasoning over answer sets \citep{cozman2017semantics}.
\end{definition}

\subsubsection{Maximum-Entropy Semantics}

Another widely used semantics is the Maximum Entropy (\textsc{MaxEnt}) approach.
It distributes probability mass as uniformly as possible across stable models.

\begin{definition}[Maximum Entropy (MaxEnt) Semantics]
    For each total choice $\theta$, let $n$ be the number of stable models in
    $\Gamma(\theta)$. The \textsc{MaxEnt} semantics assigns to a stable model
    $\mathcal{M}$ the probability
    \[
        \mathbb{P}(\mathcal{M}) =
        \sum_{\theta : \mathcal{M} \in \Gamma(\theta)} \frac{\mathbb{P}(\theta)}{n}.
    \]
    In other words, the probability mass of every total choice is divided equally
    among the stable models that it induces \citep{cozman2017semantics}.
\end{definition}

\subsection{PASP Inference as 2AMC}

The credal and \textsc{MaxEnt} semantics align naturally with the two-level
weighted model counting (2AMC) framework described in
Chapter~\ref{ch:amc}. Both can be encoded as 2AMC instances
\citep{kiesel2023knowledge, azzolini2023inference}. For \textsc{MaxEnt}, the
inner counting problem enumerates stable models for each total choice, while the
outer aggregation sums their probabilities \citep{kiesel2023knowledge}. For the
credal semantics, the inner level checks whether the query holds in some or all
stable models—capturing brave or cautious reasoning—and the outer level again
aggregates probabilities in a weighted model count \citep{azzolini2023inference}.

%---------------------------------------------------------------
